{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka8T9ybzaqFD",
        "outputId": "c81017e7-0dd4-4e2d-81d3-1feec8dde733"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading COCO validation images (5000 images)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "val2017.zip: 100%|██████████| 778M/778M [00:18<00:00, 44.0MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading COCO annotations...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "annotations_trainval2017.zip: 100%|██████████| 241M/241M [00:05<00:00, 49.2MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting validation images...\n",
            "Extracting annotations...\n",
            "COCO validation set downloaded successfully!\n",
            "Found 5000 images in ./coco_data/val2017\n",
            "Annotations file contains information for 5000 images and 36781 annotations\n",
            "Dataset verification completed successfully!\n",
            "\n",
            "Dataset information:\n",
            "- Images directory: ./coco_data/val2017\n",
            "- Annotations file: ./coco_data/annotations/instances_val2017.json\n",
            "\n",
            "The COCO validation set (5000 images) with annotations is ready for evaluation!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import json\n",
        "\n",
        "def download_file(url, destination):\n",
        "    response = requests.get(url, stream=True)\n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "    block_size = 1024  # 1 Kibibyte\n",
        "\n",
        "    with open(destination, 'wb') as file, tqdm(\n",
        "            desc=os.path.basename(destination),\n",
        "            total=total_size,\n",
        "            unit='iB',\n",
        "            unit_scale=True,\n",
        "            unit_divisor=1024,\n",
        "        ) as bar:\n",
        "        for data in response.iter_content(block_size):\n",
        "            size = file.write(data)\n",
        "            bar.update(size)\n",
        "\n",
        "def download_coco_validation_set(root_dir='./coco_data'):\n",
        "    os.makedirs(root_dir, exist_ok=True)\n",
        "\n",
        "    # Check if already downloaded\n",
        "    if os.path.exists(os.path.join(root_dir, 'val2017')) and \\\n",
        "       os.path.exists(os.path.join(root_dir, 'annotations')):\n",
        "        print(\"COCO validation set already exists!\")\n",
        "        return os.path.join(root_dir, 'val2017'), os.path.join(root_dir, 'annotations/instances_val2017.json')\n",
        "\n",
        "    # URLs for downloading\n",
        "    val_images_url = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
        "    annotations_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
        "\n",
        "    # Download validation images\n",
        "    print(\"Downloading COCO validation images (5000 images)...\")\n",
        "    val_zip_path = os.path.join(root_dir, \"val2017.zip\")\n",
        "    download_file(val_images_url, val_zip_path)\n",
        "\n",
        "    # Download annotations\n",
        "    print(\"ddownloading COCO annotations...\")\n",
        "    ann_zip_path = os.path.join(root_dir, \"annotations_trainval2017.zip\")\n",
        "    download_file(annotations_url, ann_zip_path)\n",
        "\n",
        "    # Extract validation images\n",
        "    print(\"extracting validation images...\")\n",
        "    with zipfile.ZipFile(val_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(root_dir)\n",
        "\n",
        "    # Extract annotations\n",
        "    print(\"extracting annotations...\")\n",
        "    with zipfile.ZipFile(ann_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(root_dir)\n",
        "\n",
        "    # Clean up zip files\n",
        "    os.remove(val_zip_path)\n",
        "    os.remove(ann_zip_path)\n",
        "\n",
        "    print(\"COCO validation set downloaded successfully!\")\n",
        "    return os.path.join(root_dir, 'val2017'), os.path.join(root_dir, 'annotations/instances_val2017.json')\n",
        "\n",
        "def verify_coco_dataset(images_dir, annotations_file):\n",
        "    # Check if images directory exists and has files\n",
        "    if not os.path.exists(images_dir):\n",
        "        print(f\"Error: Images directory {images_dir} does not exist\")\n",
        "        return False\n",
        "\n",
        "    image_files = [f for f in os.listdir(images_dir) if f.endswith('.jpg')]\n",
        "    print(f\"Found {len(image_files)} images in {images_dir}\")\n",
        "\n",
        "    # Check if annotations file exists and is valid JSON\n",
        "    if not os.path.exists(annotations_file):\n",
        "        print(f\"Error: Annotations file {annotations_file} does not exist\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        with open(annotations_file, 'r') as f:\n",
        "            annotations = json.load(f)\n",
        "            num_images = len(annotations['images'])\n",
        "            num_annotations = len(annotations['annotations'])\n",
        "            print(f\"Annotations file contains information for {num_images} images and {num_annotations} annotations\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading annotations file: {e}\")\n",
        "        return False\n",
        "\n",
        "    print(\"Dataset verification completed successfully!\")\n",
        "    return True\n",
        "\n",
        "images_dir, annotations_file = download_coco_validation_set()\n",
        "\n",
        "# Verify the dataset\n",
        "verify_coco_dataset(images_dir, annotations_file)\n",
        "\n",
        "print(\"\\nDataset information:\")\n",
        "print(f\"- Images directory: {images_dir}\")\n",
        "print(f\"- Annotations file: {annotations_file}\")\n",
        "print(\"\\nThe COCO validation set (5000 images) with annotations is ready for evaluation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zED2hVsav6c",
        "outputId": "e50543a1-00db-4001-daa8-7e5452c8893c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.73s)\n",
            "creating index...\n",
            "index created!\n",
            "Created data loader with 5000 images for evaluation\n",
            "Number of images: 5000\n",
            "Number of categories: 80\n",
            "\n",
            "Some COCO categories:\n",
            "  1. person\n",
            "  2. bicycle\n",
            "  3. car\n",
            "  4. motorcycle\n",
            "  5. airplane\n",
            "  6. bus\n",
            "  7. train\n",
            "  8. truck\n",
            "  9. boat\n",
            "  10. traffic light\n",
            "  11. fire hydrant\n",
            "  12. stop sign\n",
            "  13. parking meter\n",
            "  14. bench\n",
            "  15. bird\n",
            "  16. cat\n",
            "  17. dog\n",
            "  18. horse\n",
            "  19. sheep\n",
            "  20. cow\n",
            "  21. elephant\n",
            "  22. bear\n",
            "  23. zebra\n",
            "  24. giraffe\n",
            "  25. backpack\n",
            "  26. umbrella\n",
            "  27. handbag\n",
            "  28. tie\n",
            "  29. suitcase\n",
            "  30. frisbee\n",
            "  31. skis\n",
            "  32. snowboard\n",
            "  33. sports ball\n",
            "  34. kite\n",
            "  35. baseball bat\n",
            "  36. baseball glove\n",
            "  37. skateboard\n",
            "  38. surfboard\n",
            "  39. tennis racket\n",
            "  40. bottle\n",
            "  41. wine glass\n",
            "  42. cup\n",
            "  43. fork\n",
            "  44. knife\n",
            "  45. spoon\n",
            "  46. bowl\n",
            "  47. banana\n",
            "  48. apple\n",
            "  49. sandwich\n",
            "  50. orange\n",
            "  51. broccoli\n",
            "  52. carrot\n",
            "  53. hot dog\n",
            "  54. pizza\n",
            "  55. donut\n",
            "  56. cake\n",
            "  57. chair\n",
            "  58. couch\n",
            "  59. potted plant\n",
            "  60. bed\n",
            "  61. dining table\n",
            "  62. toilet\n",
            "  63. tv\n",
            "  64. laptop\n",
            "  65. mouse\n",
            "  66. remote\n",
            "  67. keyboard\n",
            "  68. cell phone\n",
            "  69. microwave\n",
            "  70. oven\n",
            "  71. toaster\n",
            "  72. sink\n",
            "  73. refrigerator\n",
            "  74. book\n",
            "  75. clock\n",
            "  76. vase\n",
            "  77. scissors\n",
            "  78. teddy bear\n",
            "  79. hair drier\n",
            "  80. toothbrush\n",
            "\n",
            "Dataset is ready for evaluation!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from pycocotools.coco import COCO\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class COCOEvalDataset(Dataset):\n",
        "    \"\"\"COCO dataset for evaluation with annotations\"\"\"\n",
        "    def __init__(self, root, annFile,transform=None):\n",
        "        self.root = root\n",
        "        self.coco = COCO(annFile)\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "\n",
        "        # Get categories\n",
        "        self.categories = self.coco.loadCats(self.coco.getCatIds())\n",
        "        self.categories.sort(key=lambda x: x['id'])\n",
        "        self.category_ids = [category['id'] for category in self.categories]\n",
        "        self.category_names = [category['name'] for category in self.categories]\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        img_id = self.ids[idx]\n",
        "        img_info = self.coco.loadImgs(img_id)[0]\n",
        "        img_path = os.path.join(self.root, img_info['file_name'])\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Apply transformations that match the pretrained models\n",
        "        transform = T.Compose([\n",
        "            T.ToTensor(),\n",
        "            # T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        image = transform(img)\n",
        "\n",
        "        # Get annotations\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        areas = []\n",
        "        iscrowd = []\n",
        "\n",
        "        for ann in anns:\n",
        "            x, y, w, h = ann['bbox']\n",
        "            # Convert to [x1, y1, x2, y2] format\n",
        "            boxes.append([x, y, (x + w), (y + h)])\n",
        "\n",
        "            # Map COCO category_id to model index\n",
        "            # Handle case where category might not be in our mapping\n",
        "            cat_id = ann['category_id']\n",
        "            if cat_id in self.category_ids:\n",
        "                labels.append(cat_id)\n",
        "            else:\n",
        "                # Skip annotations with unknown categories\n",
        "                continue\n",
        "\n",
        "            areas.append(ann['area'])\n",
        "            iscrowd.append(ann['iscrowd'])\n",
        "\n",
        "        # Convert to tensors\n",
        "        if boxes:\n",
        "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "            areas = torch.as_tensor(areas, dtype=torch.float32)\n",
        "            iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
        "        else:\n",
        "            # Handle images with no annotations\n",
        "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "            labels = torch.zeros((0,), dtype=torch.int64)\n",
        "            areas = torch.zeros((0,), dtype=torch.float32)\n",
        "            iscrowd = torch.zeros((0,), dtype=torch.int64)\n",
        "\n",
        "        # Create target dict\n",
        "        target = {\n",
        "            'boxes': boxes,\n",
        "            'labels': labels,\n",
        "            'image_id': torch.tensor([img_id]),\n",
        "            'area': areas,\n",
        "            'iscrowd': iscrowd\n",
        "        }\n",
        "\n",
        "        return image, target\n",
        "\n",
        "def create_coco_loader(images_dir, annotations_file, batch_size=1, num_workers=4):\n",
        "    \"\"\"Create a data loader for COCO evaluation\"\"\"\n",
        "    # Define transforms\n",
        "    transform = T.Compose([\n",
        "        # T.Resize((300, 300)),  # Resize to 300x300\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = COCOEvalDataset(\n",
        "        root=images_dir,\n",
        "        annFile=annotations_file,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    # Create data loader\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=lambda x: x  # Keep variable sized tensors\n",
        "    )\n",
        "\n",
        "\n",
        "    print(f\"Created data loader with {len(dataset)} images for evaluation\")\n",
        "    return loader, dataset\n",
        "\n",
        "images_dir = \"./coco_data/val2017\"\n",
        "annotations_file = \"./coco_data/annotations/instances_val2017.json\"\n",
        "\n",
        "# Create data loader\n",
        "loader, dataset = create_coco_loader(images_dir, annotations_file)\n",
        "\n",
        "# Print some dataset statistics\n",
        "print(f\"Number of images: {len(dataset)}\")\n",
        "print(f\"Number of categories: {len(dataset.categories)}\")\n",
        "\n",
        "# Display some category names\n",
        "print(\"\\nSome COCO categories:\")\n",
        "for i, name in enumerate(dataset.category_names):\n",
        "    print(f\"  {i+1}. {name}\")\n",
        "\n",
        "print(\"\\nDataset is ready for evaluation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3rBCyN8a4Dh",
        "outputId": "4f6ebfcd-2492-46c8-9fd1-a038e7efce7e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/retinanet_resnet50_fpn_v2_coco-5905b1c5.pth\" to /root/.cache/torch/hub/checkpoints/retinanet_resnet50_fpn_v2_coco-5905b1c5.pth\n",
            "100%|██████████| 146M/146M [00:00<00:00, 170MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.75s)\n",
            "creating index...\n",
            "index created!\n",
            "Created data loader with 5000 images for evaluation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved detection visualizations to coco_detection_examples.png\n",
            "Running inference...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [10:37<00:00,  7.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running custom evaluation...\n",
            "\n",
            "Evaluation Results:\n",
            "mAP @ IoU=0.5: 0.5978\n",
            "\n",
            "AP by category:\n",
            "Category 1: 0.7571\n",
            "Category 2: 0.5649\n",
            "Category 3: 0.6503\n",
            "Category 4: 0.6962\n",
            "Category 5: 0.8572\n",
            "Category 6: 0.8100\n",
            "Category 7: 0.8342\n",
            "Category 8: 0.5587\n",
            "Category 9: 0.5150\n",
            "Category 10: 0.5399\n",
            "Category 11: 0.8495\n",
            "Category 13: 0.7079\n",
            "Category 14: 0.6213\n",
            "Category 15: 0.3814\n",
            "Category 16: 0.5174\n",
            "Category 17: 0.8812\n",
            "Category 18: 0.8078\n",
            "Category 19: 0.7882\n",
            "Category 20: 0.7620\n",
            "Category 21: 0.7566\n",
            "Category 22: 0.8267\n",
            "Category 23: 0.8949\n",
            "Category 24: 0.8615\n",
            "Category 25: 0.8691\n",
            "Category 27: 0.3226\n",
            "Category 28: 0.6113\n",
            "Category 31: 0.3025\n",
            "Category 32: 0.5225\n",
            "Category 33: 0.5877\n",
            "Category 34: 0.8174\n",
            "Category 35: 0.4750\n",
            "Category 36: 0.5419\n",
            "Category 37: 0.6090\n",
            "Category 38: 0.5920\n",
            "Category 39: 0.5602\n",
            "Category 40: 0.6384\n",
            "Category 41: 0.7810\n",
            "Category 42: 0.5864\n",
            "Category 43: 0.7733\n",
            "Category 44: 0.5520\n",
            "Category 46: 0.5700\n",
            "Category 47: 0.5830\n",
            "Category 48: 0.5067\n",
            "Category 49: 0.3002\n",
            "Category 50: 0.3033\n",
            "Category 51: 0.5643\n",
            "Category 52: 0.3863\n",
            "Category 53: 0.3324\n",
            "Category 54: 0.5415\n",
            "Category 55: 0.4087\n",
            "Category 56: 0.4217\n",
            "Category 57: 0.3523\n",
            "Category 58: 0.4943\n",
            "Category 59: 0.6799\n",
            "Category 60: 0.5942\n",
            "Category 61: 0.5232\n",
            "Category 62: 0.4534\n",
            "Category 63: 0.6071\n",
            "Category 64: 0.4572\n",
            "Category 65: 0.6414\n",
            "Category 67: 0.4340\n",
            "Category 70: 0.7619\n",
            "Category 72: 0.7431\n",
            "Category 73: 0.7512\n",
            "Category 74: 0.7827\n",
            "Category 75: 0.5012\n",
            "Category 76: 0.6839\n",
            "Category 77: 0.5597\n",
            "Category 78: 0.7621\n",
            "Category 79: 0.5276\n",
            "Category 80: 0.5013\n",
            "Category 81: 0.5869\n",
            "Category 82: 0.6983\n",
            "Category 84: 0.2711\n",
            "Category 85: 0.7691\n",
            "Category 86: 0.5613\n",
            "Category 87: 0.4255\n",
            "Category 88: 0.6676\n",
            "Category 89: 0.1550\n",
            "Category 90: 0.3764\n",
            "Saved precision-recall curves to 'precision_recall_curves.png'\n"
          ]
        }
      ],
      "source": [
        "# from torchvision.io.image import decode_image\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights,FasterRCNN_MobileNet_V3_Large_FPN_Weights,retinanet_resnet50_fpn_v2,RetinaNet_ResNet50_FPN_V2_Weights\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "# from coco_loader import create_coco_loader\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import torchvision\n",
        "import torch\n",
        "import torchvision\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "# from coco_loader import create_coco_loader\n",
        "from PIL import Image\n",
        "import zipfile\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from torchvision.ops import nms\n",
        "\n",
        "def calculate_iou(box1, box2):\n",
        "    \"\"\"\n",
        "    Calculate IoU between two bounding boxes\n",
        "    Box format: [x1, y1, x2, y2]\n",
        "    \"\"\"\n",
        "    # Calculate intersection area\n",
        "    x_left = max(box1[0], box2[0])\n",
        "    y_top = max(box1[1], box2[1])\n",
        "    x_right = min(box1[2], box2[2])\n",
        "    y_bottom = min(box1[3], box2[3])\n",
        "\n",
        "    if x_right < x_left or y_bottom < y_top:\n",
        "        return 0.0\n",
        "\n",
        "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
        "\n",
        "    # Calculate union area\n",
        "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "    union_area = box1_area + box2_area - intersection_area\n",
        "\n",
        "    return intersection_area / union_area if union_area > 0 else 0.0\n",
        "\n",
        "def convert_to_xywh(box):\n",
        "    \"\"\"Convert box from [x1, y1, x2, y2] to [x, y, width, height]\"\"\"\n",
        "    return [box[0], box[1], box[2] - box[0], box[3] - box[1]]\n",
        "\n",
        "def convert_to_xyxy(box):\n",
        "    \"\"\"Convert box from [x, y, width, height] to [x1, y1, x2, y2]\"\"\"\n",
        "    return [box[0], box[1], box[0] + box[2], box[1] + box[3]]\n",
        "\n",
        "def compute_ap(precision, recall):\n",
        "    \"\"\"Compute Average Precision using 11-point interpolation\"\"\"\n",
        "    ap = 0.0\n",
        "    for t in np.arange(0.0, 1.1, 0.1):\n",
        "        if np.sum(recall >= t) == 0:\n",
        "            p = 0\n",
        "        else:\n",
        "            p = np.max(precision[recall >= t])\n",
        "        ap += p / 11.0\n",
        "    return ap\n",
        "\n",
        "def evaluate_detections(predictions, ground_truth, iou_threshold=0.5, max_dets=100):\n",
        "    # Group predictions and ground truth by image_id and category_id\n",
        "    pred_by_img = defaultdict(list)\n",
        "    gt_by_img = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "    # Get all category ids\n",
        "    category_ids = set()\n",
        "\n",
        "    # Process predictions\n",
        "    for pred in predictions:\n",
        "        img_id = pred['image_id']\n",
        "        cat_id = pred['category_id']\n",
        "        pred_by_img[img_id].append(pred)\n",
        "        category_ids.add(cat_id)\n",
        "\n",
        "    # Process ground truth\n",
        "    for gt in ground_truth:\n",
        "        img_id = gt['image_id']\n",
        "        cat_id = gt['category_id']\n",
        "        gt_by_img[img_id][cat_id].append(gt)\n",
        "        category_ids.add(cat_id)\n",
        "\n",
        "    # Metrics storage\n",
        "    metrics = {\n",
        "        'precision': {},\n",
        "        'recall': {},\n",
        "        'ap': {},\n",
        "        'f1_score': {}\n",
        "    }\n",
        "\n",
        "    # Calculate metrics for each category\n",
        "    for cat_id in category_ids:\n",
        "        true_positives = []\n",
        "        false_positives = []\n",
        "        scores = []\n",
        "        num_gt = 0\n",
        "\n",
        "        # Count total ground truth for this category\n",
        "        for img_id in gt_by_img:\n",
        "            num_gt += len(gt_by_img[img_id].get(cat_id, []))\n",
        "\n",
        "        # Process each image\n",
        "        for img_id in pred_by_img:\n",
        "            # Get predictions for this image\n",
        "            img_preds = [p for p in pred_by_img[img_id] if p['category_id'] == cat_id]\n",
        "\n",
        "            # Sort predictions by score in descending order\n",
        "            img_preds = sorted(img_preds, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "            # Limit number of detections\n",
        "            img_preds = img_preds[:max_dets]\n",
        "\n",
        "            # Get ground truth for this image and category\n",
        "            img_gts = gt_by_img[img_id].get(cat_id, [])\n",
        "\n",
        "            # Mark each ground truth as matched or not\n",
        "            matched_gt = [False] * len(img_gts)\n",
        "\n",
        "            # Check each prediction\n",
        "            for pred in img_preds:\n",
        "                pred_bbox = convert_to_xyxy(pred['bbox'])\n",
        "                pred_score = pred['score']\n",
        "\n",
        "                best_iou = 0\n",
        "                best_gt_idx = -1\n",
        "\n",
        "                # Find best matching ground truth\n",
        "                for gt_idx, gt in enumerate(img_gts):\n",
        "                    if matched_gt[gt_idx]:\n",
        "                        continue\n",
        "\n",
        "                    gt_bbox = convert_to_xyxy(gt['bbox'])\n",
        "                    iou = calculate_iou(pred_bbox, gt_bbox)\n",
        "\n",
        "                    if iou > best_iou:\n",
        "                        best_iou = iou\n",
        "                        best_gt_idx = gt_idx\n",
        "\n",
        "                # Check if we have a match\n",
        "                if best_iou >= iou_threshold and best_gt_idx >= 0:\n",
        "                    true_positives.append(1)\n",
        "                    false_positives.append(0)\n",
        "                    matched_gt[best_gt_idx] = True\n",
        "                else:\n",
        "                    true_positives.append(0)\n",
        "                    false_positives.append(1)\n",
        "\n",
        "                scores.append(pred_score)\n",
        "\n",
        "        # Sort by score\n",
        "        inds = np.argsort(scores)[::-1]\n",
        "        true_positives = np.array(true_positives)[inds]\n",
        "        false_positives = np.array(false_positives)[inds]\n",
        "\n",
        "        # Compute cumulative sum\n",
        "        tp_cumsum = np.cumsum(true_positives)\n",
        "        fp_cumsum = np.cumsum(false_positives)\n",
        "\n",
        "        # Compute precision and recall\n",
        "        precision = tp_cumsum / (tp_cumsum + fp_cumsum + np.finfo(float).eps)\n",
        "        recall = tp_cumsum / (num_gt + np.finfo(float).eps)\n",
        "\n",
        "        # Compute AP\n",
        "        ap = compute_ap(precision, recall)\n",
        "\n",
        "        # Store metrics\n",
        "        metrics['precision'][cat_id] = precision\n",
        "        metrics['recall'][cat_id] = recall\n",
        "        metrics['ap'][cat_id] = ap\n",
        "\n",
        "        # Compute F1 score (optional)\n",
        "        if len(precision) > 0 and len(recall) > 0:\n",
        "            f1 = 2 * precision * recall / (precision + recall + np.finfo(float).eps)\n",
        "            metrics['f1_score'][cat_id] = np.max(f1)\n",
        "        else:\n",
        "            metrics['f1_score'][cat_id] = 0.0\n",
        "\n",
        "    # Compute mAP\n",
        "    metrics['mAP'] = np.mean([metrics['ap'][cat_id] for cat_id in metrics['ap']])\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def main():\n",
        "    # Initialize model\n",
        "\n",
        "    # model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1)\n",
        "    # weights= FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1\n",
        "    # weights = FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1\n",
        "    # model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.5)\n",
        "    model=retinanet_resnet50_fpn_v2(weights=RetinaNet_ResNet50_FPN_V2_Weights.COCO_V1)\n",
        "    weights=RetinaNet_ResNet50_FPN_V2_Weights.COCO_V1\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    # Initialize preprocessing\n",
        "    preprocess = weights.transforms()\n",
        "\n",
        "    # Load COCO dataset\n",
        "    images_dir = \"./coco_data/val2017\"\n",
        "    annotations_file = \"./coco_data/annotations/instances_val2017.json\"\n",
        "    loader, dataset = create_coco_loader(images_dir, annotations_file)\n",
        "\n",
        "    # Run inference\n",
        "    results = []\n",
        "    ground_truth = []\n",
        "    img,target=dataset[0]\n",
        "    visualize_detections(model,dataset)\n",
        "\n",
        "    print(\"Running inference...\")\n",
        "    with torch.no_grad():\n",
        "        for i, (img, target) in enumerate(tqdm(dataset)):\n",
        "            if i >= 5000:  # Limit to first 100 images for testing\n",
        "                break\n",
        "\n",
        "            image_id = target['image_id'].item()\n",
        "\n",
        "            # Add ground truth for this image\n",
        "            for obj_idx in range(len(target['boxes'])):\n",
        "                gt = {\n",
        "                    'image_id': image_id,\n",
        "                    'category_id': target['labels'][obj_idx].item(),\n",
        "                    'bbox': convert_to_xywh(target['boxes'][obj_idx].tolist()),\n",
        "                    'iscrowd': 0\n",
        "                }\n",
        "                ground_truth.append(gt)\n",
        "\n",
        "            # Run model\n",
        "            batch = [preprocess(img.to('cuda'))]\n",
        "            prediction = model(batch)[0]\n",
        "\n",
        "            # Extract predictions\n",
        "            boxes = prediction['boxes'].cpu().numpy()\n",
        "            scores = prediction['scores'].cpu().numpy()\n",
        "            labels = prediction['labels'].cpu().numpy()\n",
        "\n",
        "            # Convert predictions to COCO format\n",
        "            for box, score, label in zip(boxes, scores, labels):\n",
        "                if score >= 0.05:  # Minimum score threshold\n",
        "                    # print(score)\n",
        "                    x1, y1, x2, y2 = box.tolist()\n",
        "                    coco_box = [x1, y1, x2 - x1, y2 - y1]  # [x, y, width, height]\n",
        "\n",
        "                    category_id = label.item()\n",
        "\n",
        "                    results.append({\n",
        "                        'image_id': image_id,\n",
        "                        'category_id': category_id,\n",
        "                        'bbox': coco_box,\n",
        "                        'score': float(score)\n",
        "                    })\n",
        "\n",
        "\n",
        "    # Save results to file\n",
        "    with open('custom_results.json', 'w') as f:\n",
        "        json.dump(results, f)\n",
        "\n",
        "    # Run custom evaluation\n",
        "    print(\"Running custom evaluation...\")\n",
        "    metrics = evaluate_detections(results, ground_truth)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    print(f\"mAP @ IoU={0.5}: {metrics['mAP']:.4f}\")\n",
        "\n",
        "    # Print AP for each category\n",
        "    print(\"\\nAP by category:\")\n",
        "    for cat_id in sorted(metrics['ap'].keys()):\n",
        "        print(f\"Category {cat_id}: {metrics['ap'][cat_id]:.4f}\")\n",
        "\n",
        "    # Optional: Plot precision-recall curves\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # Plot precision-recall curve for first 5 categories\n",
        "        plt.figure(figsize=(10, 8))\n",
        "\n",
        "        for i, cat_id in enumerate(sorted(list(metrics['precision'].keys()))[:5]):\n",
        "            if len(metrics['precision'][cat_id]) > 0 and len(metrics['recall'][cat_id]) > 0:\n",
        "                plt.plot(\n",
        "                    metrics['recall'][cat_id],\n",
        "                    metrics['precision'][cat_id],\n",
        "                    label=f'Category {cat_id} (AP: {metrics[\"ap\"][cat_id]:.4f})'\n",
        "                )\n",
        "\n",
        "        plt.xlabel('Recall')\n",
        "        plt.ylabel('Precision')\n",
        "        plt.title('Precision-Recall Curves')\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.savefig('precision_recall_curves.png')\n",
        "        plt.close()\n",
        "        print(\"Saved precision-recall curves to 'precision_recall_curves.png'\")\n",
        "    except ImportError:\n",
        "        print(\"Matplotlib not installed. Skipping precision-recall curve plotting.\")\n",
        "\n",
        "\n",
        "\n",
        "def visualize_detections(model, dataset, indices=None, n_examples=5, confidence_threshold=0.5):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    indices = np.random.choice(len(dataset), n_examples, replace=False)\n",
        "\n",
        "    plt.figure(figsize=(15, n_examples * 5))\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        img, target = dataset[idx]\n",
        "\n",
        "        # Create a copy of the image for drawing\n",
        "        img_np = img.permute(1, 2, 0).cpu().numpy()\n",
        "        # Denormalize\n",
        "        img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "        img_np = np.clip(img_np, 0, 1)\n",
        "\n",
        "        # Make prediction\n",
        "        with torch.no_grad():\n",
        "            prediction = model([img.to(device)])[0]\n",
        "\n",
        "        # Plot the image\n",
        "        plt.subplot(n_examples, 1, i + 1)\n",
        "        plt.imshow(img_np)\n",
        "\n",
        "\n",
        "        for box, label in zip(target['boxes'], target['labels']):\n",
        "            x, y, w, h = box[0].item(), box[1].item(), box[2].item() - box[0].item(), box[3].item() - box[1].item()\n",
        "            rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='g', facecolor='none')\n",
        "            plt.gca().add_patch(rect)\n",
        "            category_id = label.item()\n",
        "            category_name = next(cat['name'] for cat in dataset.categories if cat['id'] == category_id)\n",
        "            plt.text(x, y-5, category_name, color='g', fontsize=10, backgroundcolor='w')\n",
        "\n",
        "\n",
        "        for box, label, score in zip(prediction['boxes'], prediction['labels'], prediction['scores']):\n",
        "            if score >= confidence_threshold:\n",
        "                x, y, w, h = box[0].item(), box[1].item(), box[2].item() - box[0].item(), box[3].item() - box[1].item()\n",
        "                rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none')\n",
        "                plt.gca().add_patch(rect)\n",
        "\n",
        "\n",
        "                idx_to_cat = {v: k for k, v in dataset.category_id_to_idx.items()}\n",
        "                category_id = label.item()\n",
        "                category_name = next((cat['name'] for cat in dataset.categories if cat['id'] == category_id), \"unknown\")\n",
        "\n",
        "                plt.text(x, y+h+15, f\"{category_name}: {score:.2f}\", color='r', fontsize=10, backgroundcolor='w')\n",
        "\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"Image {target['image_id'].item()}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('coco_detection_examples.png')\n",
        "    plt.close()\n",
        "    print(f\"Saved detection visualizations to coco_detection_examples.png\")\n",
        "\n",
        "torch.set_default_device('cuda')\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
